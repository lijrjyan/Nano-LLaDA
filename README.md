environment setup

pip install uv
uv sync

dataset setup

pip install modelscope
mkdir -p dataset && modelscope download --dataset gongjy/minimind_dataset pretrain_hq.jsonl --local_dir ./dataset
mkdir -p dataset && modelscope download --dataset gongjy/minimind_dataset sft_mini_512.jsonl --local_dir ./dataset

pretrain AR

uv run train_pretrain.py \
  --data ./dataset/pretrain_hq.jsonl \
  --jsonl-field text \
  --tokenizer-dir . \
  --run-name minimind_pretrain \
  --hidden-size 512 \
  --num-hidden-layers 8 \
  --num-attention-heads 8 \
  --max-seq-len 256 \
  --epochs 1 \
  --batch-size 32

  eval AR
  uv run eval_minimind.py \
  --checkpoint weights/minimind_pretrain_state_dict.pt \
  --tokenizer-dir . \
  --prompt "请介绍你自己。" \
  --max-new-tokens 200\

pretrain llada

uv run diffusion.py \
  --train \
  --use-tokenizer \
  --tokenizer-dir . \
  --data ./dataset/pretrain_en_wikipedia_3g.jsonl \
  --jsonl-field text \
  --seq-len 256 \
  --hidden-size 512 \
  --num-hidden-layers 8 \
  --num-attention-heads 8 \
  --inference-rope-scaling \
  --learning-rate 4e-4 \
  --lr-schedule wsd \
  --warmup-steps 2000 \
  --lr-stable-ratio 0.4 \
  --min-lr-ratio 0.025 \
  --mask-schedule iid_t \
  --variable-length-prob 0.01 \
  --iid-mask-eps 1e-3 \
  --weight-decay 0.1 \
  --repeat-penalty-weight 0 \
  --init-from-minimind weights/minimind_pretrain_en_state_dict.pt \
  --run-name diffusion_from_ar_eq3 \
  --early-stop-patience 5 \
  --early-stop-min-delta 0.001 \
  --max-iters 40000 \
  --batch-size 128

  eval llada

  uv run eval_diffusion.py \
  --checkpoint weights/diffusion_no_v1.pt \
  --tokenizer-dir . \
  --seq-len 256 \
  --prompt "请介绍你自己。" \
  --max-new-tokens 200

  AR SFT

  uv run python train_sft_minimind.py \
  --data dataset/sft_mini_512.jsonl \
  --tokenizer-dir . \
  --load-from weights/minimind_pretrain_state_dict.pt \
  --run-name minimind_sft \
  --max-seq-len 512 \
  --batch-size 96 \
  --epochs 2

  llada SFT

  uv run python train_sft_diffusion.py \
  --data dataset/sft_open_perfectblend_1g.jsonl \
  --tokenizer-dir . \
  --load-from weights/diffusion_from_ar_eq3_3g_en.pt \
  --run-name diffusion_sft \
  --max-seq-len 512 \
  --batch-size 96 \
  --epochs 3

  eval AR SFT

  uv run python eval_sft_one_prompt.py \
  --prompt "你好，请介绍你自己。" \
  --tokenizer-dir . \
  --minimind-checkpoint weights/minimind_sft_state_dict.pt \
  --seq-len 512 \
  --max-new-tokens 128

  eval llada SFT
  uv run python eval_sft_one_prompt.py \
  --prompt "你好，请介绍你自己。" \
  --tokenizer-dir . \
  --diffusion-checkpoint weights/diffusion_sft_state_dict.pt \
  --seq-len 512 \
  --max-new-tokens 128

  eval both

  uv run python eval_sft_one_prompt.py \
  --prompt "你好，请介绍你自己。" \
  --tokenizer-dir . \
  --minimind-checkpoint weights/minimind_sft_state_dict.pt \
  --diffusion-checkpoint weights/diffusion_sft_state_dict.pt \
  --seq-len 512 \
  --max-new-tokens 128